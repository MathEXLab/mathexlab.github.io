---
layout: news
title: New Publication
subtitle: Evaluation of post-hoc interpretability methods
  in time-series classification
icon: print
date: 2023-03-30
image: /images/cover_natmach.jpg
hero_image: false
hero_height: is-small
hero_darken: true
show_sidebar: false
tags: research computational mathematics interpretability explainability
summary: |-
  Paper accepted in Nature Machine Intelligence on the evaluation
  of post-hoc interpretability for neural networks in time series
  classification tasks.
---

<html>
  <div class="content">
  Various post-hoc interpretability methods exist to evaluate the results
  of machine learning classification and prediction tasks. To better understand
  the performance and reliability of such methods, which is particularly
  necessary in high-risk applications, Turbe et al. have developed
  a framework for quantitative comparison of post-hoc interpretability
  approaches in time-series classification.</i>.

  Thanks to University of Geneva collaborators:
  <b>Hugues Turb√©</b>, <b>Mina Bjelogrcic</b>, and <b>Christian Lovis</b>,
  <br>
  <a href="https://www.nature.com/articles/s42256-023-00620-w"
     style="">
    <button class="button is-outlined is-link is-small"> <b>LINK</b> </button>
  </a>
  </div>


  <div class="content"><h4> Plain-language description </h4></div>
  <hr>
  <div class="notification is-info is-light">
    You can read more here: <a href="https://cde.nus.edu.sg/shining-a-light-into-the-black-box-of-ai/" style="">nus-cde</a>
    <br>
    and here: <a href="https://www.unige.ch/communication/communiques/en/2023/dans-la-boite-noire-des-intelligences-artificielles" style="">unige</a>
  </div>

  <div class="content"><h4> Abstract </h4></div>
  <hr>
  <div class="notification is-light">
    Post-hoc interpretability methods are critical tools to explain neural-network results. Several post-hoc methods have emerged in recent years but they produce different results when applied to a given task, raising the question of which method is the most suitable to provide accurate post-hoc interpretability. To understand the performance of each method, quantitative evaluation of interpretability methods is essential; however, currently available frameworks have several drawbacks that hinder the adoption of post-hoc interpretability methods, especially in high-risk sectors. In this work we propose a framework with quantitative metrics to assess the performance of existing post-hoc interpretability methods, particularly in time-series classification. We show that several drawbacks identified in the literature are addressed, namely, the dependence on human judgement, retraining and the shift in the data distribution when occluding samples. We also design a synthetic dataset with known discriminative features and tunable complexity. The proposed methodology and quantitative metrics can be used to understand the reliability of interpretability methods results obtained in practical applications. In turn, they can be embedded within operational workflows in critical fields that require accurate interpretability results for, example, regulatory policies.
  </div>

  <br>
</html>
